from base_tool import BaseTool, ToolMetadata, ToolResult
import requests
from bs4 import BeautifulSoup
import tempfile
import os
from typing import Dict, Any
import re
from urllib.parse import urljoin, urlparse
import time
from datetime import datetime
import sys

class SinglePaperExtractionTool(BaseTool):
    """
    å•ç¯‡è®ºæ–‡æå–å·¥å…· - ä»å•ä¸ªè®ºæ–‡URLæå–æ‘˜è¦ã€æ ‡é¢˜å’ŒPDFæ–‡ä»¶
    
    ä½œç”¨ï¼š
    1. ä½œä¸ºæœ€åŸºç¡€çš„è®ºæ–‡æå–å·¥å…·ï¼Œä¸ºå…¶ä»–å·¥å…·æä¾›æ ¸å¿ƒåŠŸèƒ½
    2. ä»HuggingFace Papersæˆ–ç±»ä¼¼ç½‘ç«™æå–å•ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
    3. ä¸‹è½½å¹¶ä¿å­˜è®ºæ–‡PDFåˆ°ä¸´æ—¶ç›®å½•
    4. æä¾›æ ‡å‡†åŒ–çš„è®ºæ–‡æ•°æ®æ ¼å¼
    """
    
    def __init__(self, log_queue=None):
        """
        åˆå§‹åŒ–å•ç¯‡è®ºæ–‡æå–å·¥å…·
        
        å‚æ•°:
            log_queue: æ—¥å¿—é˜Ÿåˆ—ï¼Œç”¨äºå‘ä¸»è¿›ç¨‹å‘é€æ—¥å¿—ä¿¡æ¯
        """
        super().__init__(log_queue)
        
        # ç½‘ç»œè¯·æ±‚é…ç½®ï¼šè¿™æ®µä»£ç æ˜¯åˆ›å»ºäº†ä¸€ä¸ªâ€œå‡è£…è‡ªå·±æ˜¯ Chrome æµè§ˆå™¨â€çš„ç½‘ç»œè¯·æ±‚å¯¹è±¡ self.sessionï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è®¿é—®ç½‘é¡µï¼Œä¾¿äºçˆ¬è™«ã€æ¥å£è°ƒç”¨ç­‰ã€‚
        self.session = requests.Session() # åˆ›å»ºäº†ä¸€ä¸ªæµè§ˆå™¨å¯¹è¯è¯·æ±‚
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # è¶…æ—¶å’Œé‡è¯•é…ç½®
        self.request_timeout = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.max_retries = 3       # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_delay = 2       # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        # PDFä¸‹è½½é…ç½®
        self.temp_pdf_dir = "temp_pdf"
        self.max_pdf_size = 50 * 1024 * 1024   # æœ€å¤§PDFæ–‡ä»¶å¤§å°ï¼ˆ50MBï¼‰ 1024bit * 1024kb * 50mb
        
        # BeautifulSoupè§£æå™¨é…ç½®
        self.parser = 'html.parser'  # é»˜è®¤è§£æå™¨
        
        # æ”¯æŒçš„è®ºæ–‡ç½‘ç«™é…ç½®
        self.supported_domains = {
            'huggingface.co': {
                'title_selector': 'h1',
                'abstract_selector': '.pb-8 p',
                'pdf_link_selector': 'a[href$=".pdf"]'
            },
            'arxiv.org': {
                'title_selector': 'h1.title',
                'abstract_selector': 'blockquote.abstract',
                'pdf_link_selector': 'a[href*="/pdf/"]'
            }
        }
        
        # åˆ›å»ºä¸´æ—¶PDFç›®å½•
        os.makedirs(self.temp_pdf_dir, exist_ok=True)
        
        # æ—¥å¿—è®°å½•
        if self.log_queue:
            self.log_queue.put("SinglePaperExtractionTool initialized successfully")
    
    def get_metadata(self) -> ToolMetadata:
        """
        è·å–å·¥å…·å…ƒæ•°æ®
        
        ä½œç”¨ï¼š
        1. å®šä¹‰å·¥å…·çš„åŸºæœ¬ä¿¡æ¯å’Œå‚æ•°è§„èŒƒ
        2. è®©Agentäº†è§£å¦‚ä½•æ­£ç¡®è°ƒç”¨è¿™ä¸ªå·¥å…·
        3. æ”¯æŒå·¥å…·çš„è‡ªåŠ¨å‘ç°å’Œåˆ†ç±»
        
        è¿”å›:
            ToolMetadata: åŒ…å«å·¥å…·åç§°ã€æè¿°ã€å‚æ•°å®šä¹‰ç­‰ä¿¡æ¯
        """
        return ToolMetadata(
            name="single_paper_extractor",
            description="ä»å•ä¸ªè®ºæ–‡URLæå–æ‘˜è¦ã€æ ‡é¢˜å’ŒPDFæ–‡ä»¶ï¼Œæ”¯æŒHuggingFace Paperså’ŒarXivç­‰ä¸»æµè®ºæ–‡ç½‘ç«™",
            parameters={
                "paper_url": {
                    "type": "str",
                    "required": True,
                    "description": "è®ºæ–‡é¡µé¢çš„å®Œæ•´URLåœ°å€ï¼Œæ”¯æŒHuggingFace Papersã€arXivç­‰ç½‘ç«™",
                    "example": "https://huggingface.co/papers/2301.07041"
                },
                "download_pdf": {
                    "type": "bool",
                    "required": False,
                    "default": True,
                    "description": "æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°ï¼Œé»˜è®¤ä¸ºTrue"
                },
                "custom_filename": {
                    "type": "str",
                    "required": False,
                    "description": "è‡ªå®šä¹‰PDFæ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨è®ºæ–‡æ ‡é¢˜"
                }
            },
            return_type="dict",
            return_description={
                "description": "åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸",
                "schema": {
                    "title": "è®ºæ–‡æ ‡é¢˜",
                    "abstract": "è®ºæ–‡æ‘˜è¦",
                    "pdf_path": "PDFæ–‡ä»¶æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœä¸‹è½½æˆåŠŸï¼‰",
                    "pdf_url": "PDFæ–‡ä»¶çš„åŸå§‹URL",
                    "url": "è®ºæ–‡é¡µé¢URL",
                    "extraction_time": "æå–æ—¶é—´æˆ³",
                    "success": "æå–æ˜¯å¦æˆåŠŸ",
                    "error_message": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"
                }
            },
            category="extraction",
            tags=["paper", "pdf", "academic", "research"],
            version="1.0.0"
        )
    
    def _execute_impl(self, **kwargs) -> Dict[str, Any]:
        """
        æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ - æå–å•ç¯‡è®ºæ–‡ä¿¡æ¯
        
        ä½œç”¨ï¼š
        1. å®ç°è®ºæ–‡ä¿¡æ¯æå–çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
        2. ä»ç½‘é¡µä¸­è§£ææ ‡é¢˜ã€æ‘˜è¦å’ŒPDFé“¾æ¥
        3. ä¸‹è½½PDFæ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        4. è¿”å›ç»“æ„åŒ–çš„è®ºæ–‡æ•°æ®
        
        å‚æ•°:
            paper_url (str): è®ºæ–‡é¡µé¢çš„URLåœ°å€
            download_pdf (bool, optional): æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            custom_filename (str, optional): è‡ªå®šä¹‰PDFæ–‡ä»¶å
            
        è¿”å›:
            Dict[str, Any]: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å®Œæ•´å­—å…¸
        """
        
        # 1. è·å–å¹¶éªŒè¯è¾“å…¥å‚æ•°
        paper_url = kwargs.get('paper_url')
        download_pdf = kwargs.get('download_pdf', True)  # é»˜è®¤ä¸‹è½½PDF
        custom_filename = kwargs.get('custom_filename', None)  # è‡ªå®šä¹‰æ–‡ä»¶å
        
        # åˆå§‹åŒ–è¿”å›ç»“æœå­—å…¸
        result = {
            "title": None,
            "abstract": None,
            "pdf_path": None,
            "pdf_url": None,
            "url": paper_url,
            "extraction_time": datetime.now().isoformat(),
            "success": False,
            "error_message": None
        }
        
        try:
            # 2. å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µå†…å®¹
            if self.log_queue:
                self.log_queue.put(f"å¼€å§‹æå–è®ºæ–‡ä¿¡æ¯: {paper_url}")
            
            # ä½¿ç”¨é…ç½®çš„sessionå‘é€è¯·æ±‚ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
            response = None
            for attempt in range(self.max_retries):
                try:
                    response = self.session.get(paper_url, timeout=self.request_timeout) # ä¼šè¿”å›ä¸€ä¸ª requests.Response å¯¹è±¡ï¼Œå®ƒä»£è¡¨äº†ä¸€æ¬¡å®Œæ•´çš„ HTTP å“åº”ï¼Œé‡Œé¢åŒ…å«äº†ä½ ä»ç½‘é¡µä¸Šæ‹¿åˆ°çš„æ‰€æœ‰æ•°æ®å’Œå…ƒä¿¡æ¯ã€‚
                    response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
                    break  # è¯·æ±‚æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                except requests.RequestException as e:
                    if attempt < self.max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                        if self.log_queue:
                            self.log_queue.put(f"è¯·æ±‚å¤±è´¥ï¼Œ{self.retry_delay}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                        time.sleep(self.retry_delay)  # ç­‰å¾…åé‡è¯•
                    else:
                        raise  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            
            # 3. ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹; soup æ˜¯ä¸€ä¸ªç»“æ„åŒ– HTML å¯¹è±¡æ ‘ï¼ˆDocument Object Modelï¼‰ï¼Œç±³å¯ä»¥ç”¨.find(), .find_all()ç­‰æ–¹æ³•å»è®¿é—®ä½ éœ€è¦çš„å…ƒç´ ã€‚
            soup = BeautifulSoup(response.text, self.parser) # response.text æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå®ƒåŒ…å«äº†ä»ç½‘é¡µä¸Šè·å–çš„æ‰€æœ‰åŸå§‹HTMLå†…å®¹ã€‚
            
            # 4. æå–è®ºæ–‡æ ‡é¢˜
            title = self._extract_title_from_soup(soup)
            result["title"] = title
            
            if self.log_queue:
                self.log_queue.put(f"æå–åˆ°æ ‡é¢˜: {title}")
            
            # 5. æå–è®ºæ–‡æ‘˜è¦
            abstract = self._extract_abstract_from_soup(soup)
            result["abstract"] = abstract
            
            if self.log_queue:
                self.log_queue.put(f"æå–åˆ°æ‘˜è¦: {abstract[:100]}..." if abstract and len(abstract) > 100 else f"æå–åˆ°æ‘˜è¦: {abstract}")
            
            # 6. æŸ¥æ‰¾å¹¶å¤„ç†PDFä¸‹è½½é“¾æ¥
            if download_pdf:
                pdf_info = self._find_and_download_pdf(soup, paper_url, custom_filename, title)
                result["pdf_path"] = pdf_info.get("pdf_path")
                result["pdf_url"] = pdf_info.get("pdf_url")
                
                if pdf_info.get("pdf_path"):
                    if self.log_queue:
                        self.log_queue.put(f"PDFä¸‹è½½æˆåŠŸ: {pdf_info.get('pdf_path')}")
                else:
                    if self.log_queue:
                        self.log_queue.put("æœªæ‰¾åˆ°æœ‰æ•ˆçš„PDFä¸‹è½½é“¾æ¥")
            
            # 7. æ ‡è®°æå–æˆåŠŸ
            result["success"] = True
            
            if self.log_queue:
                self.log_queue.put(f"è®ºæ–‡ä¿¡æ¯æå–å®Œæˆ: {title}")
            
        except requests.RequestException as e:
            # ç½‘ç»œè¯·æ±‚ç›¸å…³é”™è¯¯
            error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
            result["error_message"] = error_msg
            if self.log_queue:
                self.log_queue.put(f"é”™è¯¯: {error_msg}")
                
        except Exception as e:
            # å…¶ä»–æ‰€æœ‰é”™è¯¯
            error_msg = f"æå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            result["error_message"] = error_msg
            if self.log_queue:
                self.log_queue.put(f"é”™è¯¯: {error_msg}")
        
        # 8. è¿”å›å®Œæ•´çš„ç»“æœå­—å…¸
        return result
    
    def validate_parameters(self, **kwargs) -> bool:
        """
        éªŒè¯è¾“å…¥å‚æ•°ï¼ˆæ­¤å‡½æ•°è¿˜æœªè¢«è°ƒç”¨ï¼Œè¿™ä¸ªå‡½æ•°å¯èƒ½ä¼šåœ¨_execute_implè¿è¡Œä¹‹å‰æˆ–è€…åœ¨å¼€å¤´è°ƒç”¨ï¼Œä»¥æ­¤çŸ¥é“è¾“å…¥å‚æ•°æ˜¯å¦åˆè§„ã€‚ï¼‰
        
        ä½œç”¨ï¼š
        1. æ£€æŸ¥paper_urlå‚æ•°æ˜¯å¦å­˜åœ¨ä¸”ä¸ºå­—ç¬¦ä¸²ç±»å‹
        2. éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®
        3. æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®
        4. ç¡®ä¿å‚æ•°ç¬¦åˆå·¥å…·çš„è¦æ±‚
        
        å®ç°é€»è¾‘ï¼š
        1. æ£€æŸ¥paper_urlæ˜¯å¦å­˜åœ¨
        2. éªŒè¯paper_urlæ˜¯å¦ä¸ºå­—ç¬¦ä¸²
        3. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–urlparseéªŒè¯URLæ ¼å¼
        4. å¯é€‰ï¼šå‘é€HEADè¯·æ±‚æ£€æŸ¥URLå¯è®¿é—®æ€§
        
        è¿”å›:
            bool: å‚æ•°éªŒè¯æ˜¯å¦é€šè¿‡
        """
        
        # 1. æ£€æŸ¥å¿…éœ€å‚æ•°paper_urlæ˜¯å¦å­˜åœ¨
        if 'paper_url' not in kwargs:
            if self.log_queue:
                self.log_queue.put("é”™è¯¯: ç¼ºå°‘å¿…éœ€å‚æ•° 'paper_url'")
            return False
        
        paper_url = kwargs.get('paper_url')
        
        # 2. éªŒè¯paper_urlæ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹
        if not isinstance(paper_url, str):
            if self.log_queue:
                self.log_queue.put(f"é”™è¯¯: paper_urlå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(paper_url).__name__}")
            return False
        
        # 3. æ£€æŸ¥URLæ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦
        if not paper_url.strip():
            if self.log_queue:
                self.log_queue.put("é”™è¯¯: paper_urlä¸èƒ½ä¸ºç©º")
            return False
        
        # 4. ä½¿ç”¨urlparseéªŒè¯URLæ ¼å¼çš„åŸºæœ¬æœ‰æ•ˆæ€§
        try:
            parsed_url = urlparse(paper_url)
            # æ£€æŸ¥URLæ˜¯å¦åŒ…å«schemeï¼ˆåè®®ï¼‰å’Œnetlocï¼ˆåŸŸåï¼‰
            if not parsed_url.scheme or not parsed_url.netloc:
                if self.log_queue:
                    self.log_queue.put(f"é”™è¯¯: URLæ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘åè®®æˆ–åŸŸå: {paper_url}")
                return False
            
            # æ£€æŸ¥åè®®æ˜¯å¦ä¸ºhttpæˆ–https
            if parsed_url.scheme.lower() not in ['http', 'https']:
                if self.log_queue:
                    self.log_queue.put(f"é”™è¯¯: ä¸æ”¯æŒçš„URLåè®® '{parsed_url.scheme}'ï¼Œä»…æ”¯æŒhttpå’Œhttps")
                return False
                
        except Exception as e:
            # urlparseè§£æå¤±è´¥
            if self.log_queue:
                self.log_queue.put(f"é”™è¯¯: URLè§£æå¤±è´¥: {str(e)}")
            return False
        
        # 5. éªŒè¯å¯é€‰å‚æ•°download_pdfçš„ç±»å‹ï¼ˆå¦‚æœæä¾›ï¼‰
        download_pdf = kwargs.get('download_pdf')
        if download_pdf is not None and not isinstance(download_pdf, bool):
            if self.log_queue:
                self.log_queue.put(f"é”™è¯¯: download_pdfå¿…é¡»æ˜¯å¸ƒå°”ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(download_pdf).__name__}")
            return False
        
        # 6. éªŒè¯å¯é€‰å‚æ•°custom_filenameçš„ç±»å‹ï¼ˆå¦‚æœæä¾›ï¼‰
        custom_filename = kwargs.get('custom_filename')
        if custom_filename is not None:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹
            if not isinstance(custom_filename, str):
                if self.log_queue:
                    self.log_queue.put(f"é”™è¯¯: custom_filenameå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(custom_filename).__name__}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«éæ³•å­—ç¬¦ï¼ˆWindowså’ŒLinuxé€šç”¨çš„éæ³•å­—ç¬¦ï¼‰
            illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
            if any(char in custom_filename for char in illegal_chars):
                if self.log_queue:
                    self.log_queue.put(f"é”™è¯¯: custom_filenameåŒ…å«éæ³•å­—ç¬¦: {custom_filename}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶åé•¿åº¦æ˜¯å¦åˆç†ï¼ˆé¿å…è¿‡é•¿çš„æ–‡ä»¶åï¼‰
            if len(custom_filename.strip()) == 0:
                if self.log_queue:
                    self.log_queue.put("é”™è¯¯: custom_filenameä¸èƒ½ä¸ºç©ºå­—ç¬¦ä¸²")
                return False
            
            if len(custom_filename) > 200:
                if self.log_queue:
                    self.log_queue.put(f"é”™è¯¯: custom_filenameè¿‡é•¿ï¼ˆ{len(custom_filename)}å­—ç¬¦ï¼‰ï¼Œæœ€å¤§å…è®¸200å­—ç¬¦")
                return False
        
        # 7. å¯é€‰ï¼šæ£€æŸ¥URLçš„å¯è®¿é—®æ€§ï¼ˆå‘é€HEADè¯·æ±‚ï¼‰
        # æ³¨æ„ï¼šè¿™ä¸ªæ£€æŸ¥æ¯”è¾ƒè€—æ—¶ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯èƒ½éœ€è¦æ ¹æ®éœ€æ±‚å†³å®šæ˜¯å¦å¯ç”¨
        try:
            # å‘é€HEADè¯·æ±‚æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®ï¼Œè®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            head_response = self.session.head(paper_url, timeout=10, allow_redirects=True)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç æ˜¯å¦è¡¨ç¤ºæˆåŠŸæˆ–é‡å®šå‘
            if head_response.status_code >= 400:
                if self.log_queue:
                    self.log_queue.put(f"è­¦å‘Š: URLè¿”å›HTTPçŠ¶æ€ç  {head_response.status_code}ï¼Œå¯èƒ½æ— æ³•è®¿é—®: {paper_url}")
                # æ³¨æ„ï¼šè¿™é‡Œè¿”å›Trueè€Œä¸æ˜¯Falseï¼Œå› ä¸ºæœ‰äº›ç½‘ç«™å¯èƒ½é˜»æ­¢HEADè¯·æ±‚ä½†å…è®¸GETè¯·æ±‚
                # å®é™…çš„å¯è®¿é—®æ€§æ£€æŸ¥ä¼šåœ¨_execute_implä¸­çš„GETè¯·æ±‚æ—¶è¿›è¡Œ
            
        except requests.RequestException as e:
            # ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸é˜»æ­¢éªŒè¯é€šè¿‡
            if self.log_queue:
                self.log_queue.put(f"è­¦å‘Š: æ— æ³•éªŒè¯URLå¯è®¿é—®æ€§: {str(e)}")
            # åŒæ ·è¿”å›Trueï¼Œå› ä¸ºç½‘ç»œé—®é¢˜å¯èƒ½æ˜¯ä¸´æ—¶çš„
        
        # 8. æ‰€æœ‰éªŒè¯é€šè¿‡
        if self.log_queue:
            self.log_queue.put(f"å‚æ•°éªŒè¯é€šè¿‡: {paper_url}")
        
        return True
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨
        
        ä½œç”¨ï¼š
        1. éªŒè¯å¿…è¦çš„PythonåŒ…æ˜¯å¦å·²å®‰è£…
        2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
        3. ç¡®è®¤ä¸´æ—¶ç›®å½•æ˜¯å¦å¯å†™
        4. éªŒè¯å·¥å…·çš„è¿è¡Œç¯å¢ƒ
        
        å®ç°é€»è¾‘ï¼š
        1. æ£€æŸ¥requestsåŒ…æ˜¯å¦å¯å¯¼å…¥
        2. æ£€æŸ¥BeautifulSoupåŒ…æ˜¯å¦å¯å¯¼å…¥
        3. æµ‹è¯•ç½‘ç»œè¿æ¥ï¼ˆå¯é€‰ï¼‰
        4. æ£€æŸ¥ä¸´æ—¶ç›®å½•çš„è¯»å†™æƒé™
        
        è¿”å›:
            bool: å·¥å…·æ˜¯å¦å¯ç”¨
        """
        
        # 1. æ£€æŸ¥å¿…è¦çš„PythonåŒ…æ˜¯å¦å¯ç”¨
        try:
            # å°è¯•åˆ›å»ºä¸€ä¸ªSessionå¯¹è±¡æ¥éªŒè¯requestsåŠŸèƒ½
            test_session = requests.Session()
            if self.log_queue:
                self.log_queue.put("âœ“ requestsåŒ…æ£€æŸ¥é€šè¿‡")
                
        except ImportError as e:
            # requestsåŒ…æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥
            if self.log_queue:
                self.log_queue.put(f"âœ— requestsåŒ…ä¸å¯ç”¨: {str(e)}")
            return False
        except Exception as e:
            # requestsåŒ…å¯¼å…¥æˆåŠŸä½†åˆ›å»ºSessionå¤±è´¥
            if self.log_queue:
                self.log_queue.put(f"âœ— requestsåŒ…åŠŸèƒ½å¼‚å¸¸: {str(e)}")
            return False
        
        try:
            # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„BeautifulSoupå¯¹è±¡æ¥éªŒè¯åŠŸèƒ½
            test_soup = BeautifulSoup("<html><body><h1>test</h1></body></html>", 'html.parser')
            # éªŒè¯åŸºæœ¬è§£æåŠŸèƒ½
            if test_soup.find('h1') is None:
                raise Exception("BeautifulSoupè§£æåŠŸèƒ½å¼‚å¸¸")
            if self.log_queue:
                self.log_queue.put("âœ“ BeautifulSoupåŒ…æ£€æŸ¥é€šè¿‡")
                
        except ImportError as e:
            # BeautifulSoupåŒ…æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥
            if self.log_queue:
                self.log_queue.put(f"âœ— BeautifulSoupåŒ…ä¸å¯ç”¨: {str(e)}")
            return False
        except Exception as e:
            # BeautifulSoupåŒ…å¯¼å…¥æˆåŠŸä½†åŠŸèƒ½å¼‚å¸¸
            if self.log_queue:
                self.log_queue.put(f"âœ— BeautifulSoupåŒ…åŠŸèƒ½å¼‚å¸¸: {str(e)}")
            return False
        
        # 2. æ£€æŸ¥ä¸´æ—¶PDFç›®å½•çš„è¯»å†™æƒé™
        try:
            # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
            os.makedirs(self.temp_pdf_dir, exist_ok=True)
            
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶æ¥éªŒè¯å†™æƒé™
            test_file_path = os.path.join(self.temp_pdf_dir, "test_write_permission.tmp")
            
            # å°è¯•å†™å…¥æµ‹è¯•æ–‡ä»¶
            with open(test_file_path, 'w', encoding='utf-8') as test_file:
                test_file.write("test content for write permission")
            
            # å°è¯•è¯»å–æµ‹è¯•æ–‡ä»¶éªŒè¯è¯»æƒé™
            with open(test_file_path, 'r', encoding='utf-8') as test_file:
                content = test_file.read()
                if content != "test content for write permission":
                    raise Exception("æ–‡ä»¶è¯»å–å†…å®¹ä¸åŒ¹é…")
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove(test_file_path)
            
            if self.log_queue:
                self.log_queue.put(f"âœ“ ä¸´æ—¶ç›®å½•è¯»å†™æƒé™æ£€æŸ¥é€šè¿‡: {self.temp_pdf_dir}")
                
        except PermissionError as e:
            # æƒé™ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºç›®å½•æˆ–æ–‡ä»¶
            if self.log_queue:
                self.log_queue.put(f"âœ— ä¸´æ—¶ç›®å½•æƒé™ä¸è¶³: {str(e)}")
            return False
        except OSError as e:
            # æ“ä½œç³»ç»Ÿç›¸å…³é”™è¯¯ï¼ˆç£ç›˜ç©ºé—´ä¸è¶³ã€è·¯å¾„æ— æ•ˆç­‰ï¼‰
            if self.log_queue:
                self.log_queue.put(f"âœ— ä¸´æ—¶ç›®å½•æ“ä½œç³»ç»Ÿé”™è¯¯: {str(e)}")
            return False
        except Exception as e:
            # å…¶ä»–æ–‡ä»¶ç³»ç»Ÿç›¸å…³é”™è¯¯
            if self.log_queue:
                self.log_queue.put(f"âœ— ä¸´æ—¶ç›®å½•è®¿é—®å¼‚å¸¸: {str(e)}")
            return False
        
        # 3. æ£€æŸ¥ç½‘ç»œè¿æ¥å¯ç”¨æ€§ï¼ˆå¯é€‰ï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•ï¼‰
        try:
            # ä½¿ç”¨HEADè¯·æ±‚æµ‹è¯•ç½‘ç»œè¿æ¥ï¼Œé€‰æ‹©å¯é çš„æµ‹è¯•URL
            test_urls = [
                "https://www.google.com",  # å…¨çƒå¯è®¿é—®
                "https://httpbin.org/status/200",  # HTTPæµ‹è¯•æœåŠ¡
                "https://www.baidu.com"  # ä¸­å›½å¤§é™†å¯è®¿é—®
            ]
            
            network_available = False
            for test_url in test_urls:
                try:
                    # å‘é€HEADè¯·æ±‚ï¼Œè®¾ç½®çŸ­è¶…æ—¶æ—¶é—´
                    response = self.session.head(test_url, timeout=5)
                    if response.status_code < 400:
                        network_available = True
                        if self.log_queue:
                            self.log_queue.put(f"âœ“ ç½‘ç»œè¿æ¥æ£€æŸ¥é€šè¿‡: {test_url}")
                        break
                except:
                    # å•ä¸ªURLå¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
                    continue
            
            if not network_available:
                # æ‰€æœ‰æµ‹è¯•URLéƒ½å¤±è´¥ï¼Œä½†è¿™ä¸ä¸€å®šæ„å‘³ç€å·¥å…·ä¸å¯ç”¨
                # å› ä¸ºç›®æ ‡ç½‘ç«™å¯èƒ½ä»ç„¶å¯è®¿é—®
                if self.log_queue:
                    self.log_queue.put("âš  ç½‘ç»œè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œä½†ä¸å½±å“å·¥å…·å¯ç”¨æ€§åˆ¤æ–­")
            
        except Exception as e:
            # ç½‘ç»œæµ‹è¯•å¼‚å¸¸ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“å·¥å…·å¯ç”¨æ€§
            if self.log_queue:
                self.log_queue.put(f"âš  ç½‘ç»œè¿æ¥æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        # 4. æ£€æŸ¥å…¶ä»–ç³»ç»Ÿä¾èµ–
        try:
            # éªŒè¯æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
            test_pattern = re.compile(r'test')
            if not test_pattern.match('test'):
                raise Exception("æ­£åˆ™è¡¨è¾¾å¼åŠŸèƒ½å¼‚å¸¸")
            
            # éªŒè¯URLè§£ææ¨¡å—
            test_parsed = urlparse('https://example.com/test')
            if not test_parsed.scheme or not test_parsed.netloc:
                raise Exception("URLè§£æåŠŸèƒ½å¼‚å¸¸")
            
            # éªŒè¯æ—¶é—´å¤„ç†æ¨¡å—
            from datetime import datetime
            test_time = datetime.now()
            if not test_time:
                raise Exception("æ—¶é—´å¤„ç†åŠŸèƒ½å¼‚å¸¸")
            
            if self.log_queue:
                self.log_queue.put("âœ“ ç³»ç»Ÿä¾èµ–æ¨¡å—æ£€æŸ¥é€šè¿‡")
                
        except ImportError as e:
            # ç³»ç»Ÿæ¨¡å—å¯¼å…¥å¤±è´¥ï¼ˆè¿™ç§æƒ…å†µå¾ˆå°‘è§ï¼‰
            if self.log_queue:
                self.log_queue.put(f"âœ— ç³»ç»Ÿæ¨¡å—ä¸å¯ç”¨: {str(e)}")
            return False
        except Exception as e:
            # ç³»ç»Ÿæ¨¡å—åŠŸèƒ½å¼‚å¸¸
            if self.log_queue:
                self.log_queue.put(f"âœ— ç³»ç»Ÿæ¨¡å—åŠŸèƒ½å¼‚å¸¸: {str(e)}")
            return False
        
        # 5. éªŒè¯å·¥å…·è‡ªèº«çš„é…ç½®
        try:
            # æ£€æŸ¥sessionå¯¹è±¡æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
            if not hasattr(self, 'session') or self.session is None:
                raise Exception("ç½‘ç»œä¼šè¯å¯¹è±¡æœªåˆå§‹åŒ–")
            
            # æ£€æŸ¥å…³é”®é…ç½®å‚æ•°
            if not hasattr(self, 'temp_pdf_dir') or not self.temp_pdf_dir:
                raise Exception("ä¸´æ—¶ç›®å½•é…ç½®ç¼ºå¤±")
            
            if not hasattr(self, 'request_timeout') or self.request_timeout <= 0:
                raise Exception("è¯·æ±‚è¶…æ—¶é…ç½®æ— æ•ˆ")
            
            if not hasattr(self, 'max_retries') or self.max_retries < 0:
                raise Exception("é‡è¯•æ¬¡æ•°é…ç½®æ— æ•ˆ")
            
            if self.log_queue:
                self.log_queue.put("âœ“ å·¥å…·é…ç½®æ£€æŸ¥é€šè¿‡")
                
        except Exception as e:
            # å·¥å…·é…ç½®å¼‚å¸¸
            if self.log_queue:
                self.log_queue.put(f"âœ— å·¥å…·é…ç½®å¼‚å¸¸: {str(e)}")
            return False
        
        # 6. æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå·¥å…·å¯ç”¨
        if self.log_queue:
            self.log_queue.put("âœ… SinglePaperExtractionTool å¯ç”¨æ€§æ£€æŸ¥å…¨éƒ¨é€šè¿‡")
        
        return True
    
    def get_usage_example(self) -> Dict[str, Any]:
        """
        è·å–å·¥å…·ä½¿ç”¨ç¤ºä¾‹
        
        ä½œç”¨ï¼š
        1. ä¸ºAgentæä¾›å…·ä½“çš„ä½¿ç”¨ç¤ºä¾‹
        2. å±•ç¤ºæ­£ç¡®çš„å‚æ•°æ ¼å¼å’Œç±»å‹
        3. è¯´æ˜é¢„æœŸçš„è¾“å‡ºæ ¼å¼å’Œå­—æ®µ
        4. å¸®åŠ©Agentå­¦ä¹ å¦‚ä½•ä½¿ç”¨è¿™ä¸ªå·¥å…·
        5. æä¾›ä¸åŒåœºæ™¯ä¸‹çš„å‚æ•°é…ç½®ç¤ºä¾‹
        
        è¿”å›:
            Dict[str, Any]: åŒ…å«å®Œæ•´ä½¿ç”¨ç¤ºä¾‹çš„å­—å…¸
        """
        return {
            # åŸºæœ¬è¾“å…¥ç¤ºä¾‹
            "input_examples": {
                "basic": {
                    "paper_url": "https://arxiv.org/abs/2301.00001",
                    "description": "æœ€åŸºæœ¬çš„ä½¿ç”¨æ–¹å¼ï¼Œåªæä¾›è®ºæ–‡URL"
                },
                "with_pdf_download": {
                    "paper_url": "https://arxiv.org/abs/2301.00001",
                    "download_pdf": True,
                    "description": "å¯ç”¨PDFä¸‹è½½åŠŸèƒ½"
                },
                "with_custom_filename": {
                    "paper_url": "https://arxiv.org/abs/2301.00001",
                    "download_pdf": True,
                    "custom_filename": "my_research_paper",
                    "description": "ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶åä¿å­˜PDF"
                },
                "minimal_extraction": {
                    "paper_url": "https://arxiv.org/abs/2301.00001",
                    "download_pdf": False,
                    "description": "ä»…æå–æ ‡é¢˜å’Œæ‘˜è¦ï¼Œä¸ä¸‹è½½PDF"
                }
            },
            
            # æ”¯æŒçš„URLç±»å‹ç¤ºä¾‹
            "supported_urls": [
                "https://arxiv.org/abs/2301.00001",
                "https://huggingface.co/papers/9999.99999",
                "https://ieeexplore.ieee.org/document/9999999",
                "https://dl.acm.org/doi/10.1145/3999999.3999999",
                "https://papers.nips.cc/paper/2023/hash/abcd1234"
            ],
            
            # é¢„æœŸè¾“å‡ºæ ¼å¼
             "expected_output": {
                 "success_case": {
                     "title": "Attention Is All You Need",
                     "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...",
                     "pdf_path": "d:\\temp_pdf\\attention_is_all_you_need.pdf",
                     "pdf_url": "https://arxiv.org/pdf/1706.03762.pdf",
                     "url": "https://arxiv.org/abs/1706.03762",
                     "extraction_time": "2024-01-15T14:30:25.123456",
                     "success": True,
                     "error_message": None
                 },
                 "error_case": {
                     "title": None,
                     "abstract": None,
                     "pdf_path": None,
                     "pdf_url": None,
                     "url": "https://invalid-url.com",
                     "extraction_time": "2024-01-15T14:30:25.123456",
                     "success": False,
                     "error_message": "ç½‘ç»œè¯·æ±‚å¤±è´¥: HTTPSConnectionPool(host='invalid-url.com', port=443)"
                 }
             },
            
            # å‚æ•°è¯´æ˜
            "parameter_details": {
                "paper_url": {
                    "type": "str",
                    "required": True,
                    "description": "è®ºæ–‡é¡µé¢URLï¼Œæ”¯æŒä¸»æµå­¦æœ¯ç½‘ç«™",
                    "validation": "å¿…é¡»æ˜¯æœ‰æ•ˆçš„HTTP/HTTPS URL"
                },
                "download_pdf": {
                    "type": "bool",
                    "required": False,
                    "default": True,
                    "description": "æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°"
                },
                "custom_filename": {
                    "type": "str",
                    "required": False,
                    "default": "auto-generated from title",
                    "description": "è‡ªå®šä¹‰PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰",
                    "validation": "é•¿åº¦1-100å­—ç¬¦ï¼Œä¸å«ç‰¹æ®Šå­—ç¬¦"
                }
            },
            
            # ä½¿ç”¨åœºæ™¯
            "use_cases": [
                "ä»arXivæå–æœ€æ–°ç ”ç©¶è®ºæ–‡ä¿¡æ¯",
                "ä»Hugging Faceä¸Šè·å–daily papers"
                "æ‰¹é‡æ”¶é›†ç‰¹å®šé¢†åŸŸçš„è®ºæ–‡æ‘˜è¦",
                "ä¸ºæ–‡çŒ®ç»¼è¿°å‡†å¤‡è®ºæ–‡èµ„æ–™",
                "æ„å»ºè®ºæ–‡æ•°æ®åº“çš„åŸºç¡€æ•°æ®",
                "å­¦æœ¯ç ”ç©¶ä¸­çš„è®ºæ–‡é¢„å¤„ç†"
            ],
            
            # æ³¨æ„äº‹é¡¹
            "notes": [
                "ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ŒæŸäº›å­¦æœ¯ç½‘ç«™å¯èƒ½éœ€è¦è®¿é—®æƒé™",
                "PDFä¸‹è½½å¯èƒ½è¾ƒæ…¢ï¼Œå–å†³äºæ–‡ä»¶å¤§å°å’Œç½‘ç»œé€Ÿåº¦",
                "éƒ¨åˆ†ç½‘ç«™å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œå»ºè®®é€‚å½“å»¶æ—¶",
                "è‡ªå®šä¹‰æ–‡ä»¶åä¼šè‡ªåŠ¨è¿‡æ»¤ä¸å®‰å…¨å­—ç¬¦"
            ]
        }
    
    def cleanup(self):
        """
        æ¸…ç†å·¥å…·èµ„æº
        
        ä½œç”¨ï¼š
        1. æ¸…ç†ä¸´æ—¶ä¸‹è½½çš„PDFæ–‡ä»¶
        2. é‡Šæ”¾ç½‘ç»œè¿æ¥èµ„æº
        3. æ¸…ç†ç¼“å­˜æ•°æ®
        4. ç¡®ä¿å·¥å…·ä½¿ç”¨åä¸ç•™ä¸‹åƒåœ¾æ–‡ä»¶
        
        å®ç°é€»è¾‘ï¼š
        1. éå†ä¸´æ—¶PDFç›®å½•ï¼Œåˆ é™¤æ‰€æœ‰PDFæ–‡ä»¶
        2. æ¸…ç†ç©ºçš„ä¸´æ—¶ç›®å½•
        3. å…³é—­ç½‘ç»œSessionè¿æ¥
        4. é‡ç½®å·¥å…·çŠ¶æ€
        """
        
        # 1. æ¸…ç†ä¸´æ—¶PDFç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        try:
            if os.path.exists(self.temp_pdf_dir):
                # éå†ä¸´æ—¶ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
                for filename in os.listdir(self.temp_pdf_dir):
                    file_path = os.path.join(self.temp_pdf_dir, filename)
                    
                    # åªåˆ é™¤æ–‡ä»¶ï¼Œä¸åˆ é™¤å­ç›®å½•
                    if os.path.isfile(file_path):
                        try:
                            os.remove(file_path)
                            if self.log_queue:
                                self.log_queue.put(f"ğŸ§¹ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
                        except Exception as e:
                            if self.log_queue:
                                self.log_queue.put(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                
                # å°è¯•åˆ é™¤ç©ºçš„ä¸´æ—¶ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸ºç©ºï¼‰
                try:
                    if not os.listdir(self.temp_pdf_dir):  # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
                        os.rmdir(self.temp_pdf_dir)
                        if self.log_queue:
                            self.log_queue.put(f"ğŸ§¹ å·²åˆ é™¤ç©ºçš„ä¸´æ—¶ç›®å½•: {self.temp_pdf_dir}")
                except OSError:
                    # ç›®å½•ä¸ä¸ºç©ºæˆ–åˆ é™¤å¤±è´¥ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
                    pass
                    
        except Exception as e:
            if self.log_queue:
                self.log_queue.put(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        # 2. å…³é—­ç½‘ç»œSessionè¿æ¥
        try:
            if hasattr(self, 'session') and self.session:
                self.session.close()
                if self.log_queue:
                    self.log_queue.put("ğŸ”Œ å·²å…³é—­ç½‘ç»œSessionè¿æ¥")
        except Exception as e:
            if self.log_queue:
                self.log_queue.put(f"âš ï¸ å…³é—­ç½‘ç»œè¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        # 3. è®°å½•æ¸…ç†å®Œæˆ
        if self.log_queue:
            self.log_queue.put("âœ… SinglePaperExtractionTool èµ„æºæ¸…ç†å®Œæˆ")
    
    def delete_specific_pdf(self, pdf_path):
        """
        åˆ é™¤æŒ‡å®šçš„PDFæ–‡ä»¶
        
        ä½œç”¨ï¼š
        1. æä¾›å•ä¸ªæ–‡ä»¶åˆ é™¤åŠŸèƒ½
        2. æ”¯æŒç²¾ç¡®çš„æ–‡ä»¶æ¸…ç†
        3. å¤ç”¨Extraction.pyä¸­çš„åˆ é™¤é€»è¾‘
        
        å‚æ•°:
            pdf_path (str): è¦åˆ é™¤çš„PDFæ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        if pdf_path and os.path.exists(pdf_path):
            try:
                os.remove(pdf_path)
                if self.log_queue:
                    self.log_queue.put(f"ğŸ§¹ å·²åˆ é™¤PDFæ–‡ä»¶: {pdf_path}")
                return True
            except Exception as e:
                if self.log_queue:
                    self.log_queue.put(f"âš ï¸ åˆ é™¤PDFæ–‡ä»¶å¤±è´¥ {pdf_path}: {str(e)}")
                return False
        else:
            if self.log_queue:
                self.log_queue.put(f"âš ï¸ PDFæ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„æ— æ•ˆ: {pdf_path}")
            return False
    
    def _extract_title_from_soup(self, soup):
        """
        ä»BeautifulSoupå¯¹è±¡ä¸­æå–è®ºæ–‡æ ‡é¢˜
        
        ä½œç”¨ï¼š
        1. æä¾›æ ‡é¢˜æå–çš„ä¸“é—¨æ–¹æ³•
        2. å¤„ç†ä¸åŒç½‘ç«™çš„æ ‡é¢˜æ ¼å¼
        3. æé«˜ä»£ç çš„å¯ç»´æŠ¤æ€§
        
        å‚æ•°:
            soup: BeautifulSoupè§£æå¯¹è±¡
            
        è¿”å›:
            str: æå–åˆ°çš„è®ºæ–‡æ ‡é¢˜ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›é»˜è®¤å€¼
        """
        title = None
        
        # 1. å°è¯•ä»h1æ ‡ç­¾æå–æ ‡é¢˜ï¼ˆæœ€å¸¸è§çš„æ ‡é¢˜æ ‡ç­¾ï¼‰
        h1_tag = soup.find("h1")
        if h1_tag:
            title = h1_tag.get_text(strip=True)  # è·å–æ–‡æœ¬å¹¶å»é™¤é¦–å°¾ç©ºç™½
            if self.log_queue:
                self.log_queue.put(f"ä»h1æ ‡ç­¾æå–åˆ°æ ‡é¢˜: {title}")
        
        # 2. å¦‚æœh1æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»h3æ ‡ç­¾æå–ï¼ˆHuggingFace Paperså¸¸ç”¨æ ¼å¼ï¼‰
        if not title:
            h3_tag = soup.find("h3")
            if h3_tag:
                title = h3_tag.get_text(strip=True)
                if self.log_queue:
                    self.log_queue.put(f"ä»h3æ ‡ç­¾æå–åˆ°æ ‡é¢˜: {title}")
        
        # 3. å°è¯•ä»ç‰¹å®šclassçš„å…ƒç´ æå–ï¼ˆé’ˆå¯¹ç‰¹æ®Šç½‘ç«™æ ¼å¼ï¼‰
        if not title:
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ ‡é¢˜çš„å…¶ä»–å…ƒç´ 
            title_selectors = [
                "h2.title",  # arXivæ ¼å¼
                ".paper-title",  # é€šç”¨è®ºæ–‡æ ‡é¢˜class
                "[data-testid='paper-title']",  # æµ‹è¯•IDæ ¼å¼
                "h1.title",  # å¸¦classçš„h1æ ‡é¢˜
            ]
            
            for selector in title_selectors:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text(strip=True)
                    if self.log_queue:
                        self.log_queue.put(f"ä»é€‰æ‹©å™¨ {selector} æå–åˆ°æ ‡é¢˜: {title}")
                    break
        
        # 4. æ¸…ç†å’ŒéªŒè¯æ ‡é¢˜æ–‡æœ¬
        if title:
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
            title = re.sub(r'\s+', ' ', title).strip()
            # é™åˆ¶æ ‡é¢˜é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„æ ‡é¢˜
            if len(title) > 200:
                title = title[:200] + "..."
        else:
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤æ ‡é¢˜
            title = "æœªæ‰¾åˆ°è®ºæ–‡æ ‡é¢˜"
            if self.log_queue:
                self.log_queue.put("è­¦å‘Š: æœªèƒ½æå–åˆ°è®ºæ–‡æ ‡é¢˜")
        
        return title
    
    def _extract_abstract_from_soup(self, soup):
        """
        ä»BeautifulSoupå¯¹è±¡ä¸­æå–è®ºæ–‡æ‘˜è¦
        
        ä½œç”¨ï¼š
        1. æä¾›æ‘˜è¦æå–çš„ä¸“é—¨æ–¹æ³•
        2. å¤„ç†ä¸åŒçš„æ‘˜è¦æ ¼å¼å’Œå¸ƒå±€
        3. æ¸…ç†å’Œæ ¼å¼åŒ–æ‘˜è¦æ–‡æœ¬
        
        å‚æ•°:
            soup: BeautifulSoupè§£æå¯¹è±¡
            
        è¿”å›:
            str: æå–åˆ°çš„è®ºæ–‡æ‘˜è¦ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›é»˜è®¤å€¼
        """
        abstract_text = None
        
        # 1. æ–¹æ³•ä¸€ï¼šæŸ¥æ‰¾"Abstract"æ ‡é¢˜çš„h2æ ‡ç­¾ï¼ˆHuggingFace Papersæ ¼å¼ï¼‰
        abstract_header = soup.find("h2", string="Abstract")
        if abstract_header:
            if self.log_queue:
                self.log_queue.put("æ‰¾åˆ°Abstractæ ‡é¢˜ï¼Œå¼€å§‹æå–æ‘˜è¦å†…å®¹")
            
            # æŸ¥æ‰¾Abstractæ ‡é¢˜åçš„å…„å¼Ÿå…ƒç´ ï¼ˆåŒ…å«æ‘˜è¦å†…å®¹çš„divï¼‰
            abstract_container = abstract_header.find_next_sibling("div")
            if abstract_container:
                # åªæå–ç‰¹å®šclassçš„pæ ‡ç­¾å†…å®¹ï¼ˆHuggingFaceæ ¼å¼ï¼‰
                p_tags = abstract_container.find_all("p", class_="text-gray-600")
                if p_tags:
                    # åˆå¹¶æ‰€æœ‰pæ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
                    abstract_text = "\n".join(p.get_text(strip=True) for p in p_tags)
                    if self.log_queue:
                        self.log_queue.put(f"ä»text-gray-600ç±»æå–åˆ°æ‘˜è¦: {len(abstract_text)}å­—ç¬¦")
                
                # å¦‚æœç‰¹å®šclassæ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–æ‰€æœ‰pæ ‡ç­¾
                if not abstract_text:
                    p_tags = abstract_container.find_all("p")
                    if p_tags:
                        abstract_text = "\n".join(p.get_text(strip=True) for p in p_tags)
                        if self.log_queue:
                            self.log_queue.put(f"ä»é€šç”¨pæ ‡ç­¾æå–åˆ°æ‘˜è¦: {len(abstract_text)}å­—ç¬¦")
        
        # 2. æ–¹æ³•äºŒï¼šæŸ¥æ‰¾arXivæ ¼å¼çš„æ‘˜è¦ï¼ˆblockquote.abstractï¼‰
        if not abstract_text:
            abstract_block = soup.find("blockquote", class_="abstract")
            if abstract_block:
                abstract_text = abstract_block.get_text(strip=True)
                # ç§»é™¤"Abstract:"å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if abstract_text.startswith("Abstract:"):
                    abstract_text = abstract_text[9:].strip()
                if self.log_queue:
                    self.log_queue.put(f"ä»arXivæ ¼å¼æå–åˆ°æ‘˜è¦: {len(abstract_text)}å­—ç¬¦")
        
        # 3. æ–¹æ³•ä¸‰ï¼šæŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ‘˜è¦æ ¼å¼
        if not abstract_text:
            # å°è¯•å¤šç§å¯èƒ½çš„æ‘˜è¦é€‰æ‹©å™¨
            abstract_selectors = [
                ".abstract",  # é€šç”¨æ‘˜è¦class
                "#abstract",  # æ‘˜è¦ID
                "[data-testid='abstract']",  # æµ‹è¯•IDæ ¼å¼
                ".paper-abstract",  # è®ºæ–‡æ‘˜è¦class
                ".summary",  # æ‘˜è¦/æ€»ç»“class
            ]
            
            for selector in abstract_selectors:
                element = soup.select_one(selector)
                if element:
                    abstract_text = element.get_text(strip=True)
                    if self.log_queue:
                        self.log_queue.put(f"ä»é€‰æ‹©å™¨ {selector} æå–åˆ°æ‘˜è¦: {len(abstract_text)}å­—ç¬¦")
                    break
        
        # 4. æ–¹æ³•å››ï¼šé€šè¿‡æ–‡æœ¬å†…å®¹æŸ¥æ‰¾Abstractå…³é”®è¯
        if not abstract_text:
            # æŸ¥æ‰¾åŒ…å«"Abstract"æ–‡æœ¬çš„å…ƒç´ 
            abstract_elements = soup.find_all(text=re.compile(r"Abstract", re.IGNORECASE))
            for element in abstract_elements:
                parent = element.parent
                if parent:
                    # æŸ¥æ‰¾çˆ¶å…ƒç´ çš„ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ æˆ–å­å…ƒç´ 
                    next_element = parent.find_next_sibling()
                    if next_element:
                        potential_abstract = next_element.get_text(strip=True)
                        # éªŒè¯æ˜¯å¦åƒæ‘˜è¦ï¼ˆé•¿åº¦åˆç†ä¸”ä¸æ˜¯å¯¼èˆªæ–‡æœ¬ï¼‰
                        if 50 < len(potential_abstract) < 2000:
                            abstract_text = potential_abstract
                            if self.log_queue:
                                self.log_queue.put(f"é€šè¿‡Abstractå…³é”®è¯æŸ¥æ‰¾åˆ°æ‘˜è¦: {len(abstract_text)}å­—ç¬¦")
                            break
        
        # 5. æ¸…ç†å’ŒéªŒè¯æ‘˜è¦æ–‡æœ¬
        if abstract_text:
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
            abstract_text = re.sub(r'\s+', ' ', abstract_text).strip()
            
            # éªŒè¯æ‘˜è¦é•¿åº¦çš„åˆç†æ€§
            if len(abstract_text) < 20:
                abstract_text = "æ‘˜è¦å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æå–ä¸å®Œæ•´"
                if self.log_queue:
                    self.log_queue.put("è­¦å‘Š: æå–çš„æ‘˜è¦å†…å®¹è¿‡çŸ­")
            elif len(abstract_text) > 3000:
                # æˆªæ–­è¿‡é•¿çš„æ‘˜è¦
                abstract_text = abstract_text[:3000] + "..."
                if self.log_queue:
                    self.log_queue.put("è­¦å‘Š: æ‘˜è¦å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­")
        else:
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            abstract_text = "æœªæ‰¾åˆ°è®ºæ–‡æ‘˜è¦"
            if self.log_queue:
                self.log_queue.put("è­¦å‘Š: æœªèƒ½æå–åˆ°è®ºæ–‡æ‘˜è¦")
        
        return abstract_text
    
    def _find_and_download_pdf(self, soup, base_url, custom_filename=None, title=None):
        """
        æŸ¥æ‰¾å¹¶ä¸‹è½½PDFæ–‡ä»¶
        
        ä½œç”¨ï¼š
        1. ä»ç½‘é¡µä¸­æŸ¥æ‰¾PDFä¸‹è½½é“¾æ¥
        2. éªŒè¯PDFé“¾æ¥çš„æœ‰æ•ˆæ€§
        3. ä¸‹è½½PDFåˆ°ä¸´æ—¶ç›®å½•
        4. è¿”å›æœ¬åœ°PDFæ–‡ä»¶è·¯å¾„å’ŒåŸå§‹URL
        
        å‚æ•°:
            soup: BeautifulSoupè§£æå¯¹è±¡
            base_url: åŸºç¡€URLï¼Œç”¨äºæ„å»ºå®Œæ•´çš„PDFé“¾æ¥
            custom_filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            title: è®ºæ–‡æ ‡é¢˜ï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        è¿”å›:
            dict: åŒ…å«pdf_pathå’Œpdf_urlçš„å­—å…¸
        """
        
        result = {
            "pdf_path": None,
            "pdf_url": None
        }
        
        pdf_link = None
        
        # 1. æ–¹æ³•ä¸€ï¼šæŸ¥æ‰¾HuggingFace Papersæ ¼å¼çš„PDFé“¾æ¥
        # æŸ¥æ‰¾ç‰¹å®šclassçš„aæ ‡ç­¾ï¼ŒåŒ…å«PDFä¸‹è½½é“¾æ¥
        pdf_buttons = soup.find_all("a", class_="btn inline-flex h-9 items-center", href=True)
        for a in pdf_buttons:
            href = a["href"]
            # æ£€æŸ¥é“¾æ¥æ˜¯å¦æŒ‡å‘PDFæ–‡ä»¶
            if href.lower().endswith(".pdf") or "/pdf/" in href.lower():
                pdf_link = href
                if self.log_queue:
                    self.log_queue.put(f"ä»HuggingFaceæ ¼å¼æ‰¾åˆ°PDFé“¾æ¥: {href}")
                break
        
        # 2. æ–¹æ³•äºŒï¼šæŸ¥æ‰¾é€šç”¨çš„PDFé“¾æ¥
        if not pdf_link:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«PDFçš„é“¾æ¥
            all_links = soup.find_all("a", href=True)
            for link in all_links:
                href = link["href"]
                # æ£€æŸ¥å¤šç§PDFé“¾æ¥æ ¼å¼
                if (href.lower().endswith(".pdf") or 
                    "/pdf/" in href.lower() or 
                    "download" in href.lower() and "pdf" in href.lower()):
                    pdf_link = href
                    if self.log_queue:
                        self.log_queue.put(f"ä»é€šç”¨æ ¼å¼æ‰¾åˆ°PDFé“¾æ¥: {href}")
                    break
        
        # 3. æ–¹æ³•ä¸‰ï¼šæŸ¥æ‰¾arXivæ ¼å¼çš„PDFé“¾æ¥
        if not pdf_link:
            # arXivé€šå¸¸æœ‰ç‰¹å®šçš„PDFé“¾æ¥æ ¼å¼
            arxiv_pdf_links = soup.find_all("a", href=re.compile(r"/pdf/\d+\.\d+"))
            if arxiv_pdf_links:
                pdf_link = arxiv_pdf_links[0]["href"]
                if self.log_queue:
                    self.log_queue.put(f"ä»arXivæ ¼å¼æ‰¾åˆ°PDFé“¾æ¥: {pdf_link}")
        
        # 4. å¦‚æœæ‰¾åˆ°PDFé“¾æ¥ï¼Œè¿›è¡Œä¸‹è½½å¤„ç†
        if pdf_link:
            try:
                # æ„å»ºå®Œæ•´çš„PDF URL
                if pdf_link.startswith("http"):
                    full_pdf_url = pdf_link  # å·²ç»æ˜¯å®Œæ•´URL
                else:
                    # ç›¸å¯¹URLï¼Œéœ€è¦ä¸base_urlåˆå¹¶
                    full_pdf_url = urljoin(base_url, pdf_link)
                
                result["pdf_url"] = full_pdf_url
                
                if self.log_queue:
                    self.log_queue.put(f"å¼€å§‹éªŒè¯PDFé“¾æ¥: {full_pdf_url}")
                
                # 5. å‘é€HEADè¯·æ±‚éªŒè¯PDFæ–‡ä»¶
                head_response = self.session.head(full_pdf_url, allow_redirects=True, timeout=self.request_timeout)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€å’Œå†…å®¹ç±»å‹
                if head_response.status_code == 200:
                    content_type = head_response.headers.get("Content-Type", "").lower()
                    content_length = head_response.headers.get("Content-Length")
                    
                    # éªŒè¯æ˜¯å¦ä¸ºPDFæ–‡ä»¶
                    if "pdf" in content_type or pdf_link.lower().endswith(".pdf"):
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
                        if content_length:
                            file_size = int(content_length)
                            if file_size > self.max_pdf_size:
                                if self.log_queue:
                                    self.log_queue.put(f"PDFæ–‡ä»¶è¿‡å¤§ ({file_size} bytes)ï¼Œè·³è¿‡ä¸‹è½½")
                                return result
                        
                        if self.log_queue:
                            self.log_queue.put(f"PDFéªŒè¯æˆåŠŸï¼Œå¼€å§‹ä¸‹è½½ (å¤§å°: {content_length or 'æœªçŸ¥'} bytes)")
                        
                        # 6. ä¸‹è½½PDFæ–‡ä»¶
                        pdf_response = self.session.get(full_pdf_url, stream=True, timeout=self.request_timeout)
                        pdf_response.raise_for_status()
                        
                        # 7. ç”Ÿæˆæ–‡ä»¶å
                        if custom_filename:
                            filename = f"{custom_filename}.pdf"
                        elif title:
                            # ä½¿ç”¨è®ºæ–‡æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶åï¼Œæ¸…ç†éæ³•å­—ç¬¦
                            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]  # é™åˆ¶é•¿åº¦å¹¶æ›¿æ¢éæ³•å­—ç¬¦
                            filename = f"{safe_title}.pdf"
                        else:
                            # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆé»˜è®¤æ–‡ä»¶å
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            filename = f"paper_{timestamp}.pdf"
                        
                        # 8. åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶å†™å…¥PDFå†…å®¹
                        temp_file = tempfile.NamedTemporaryFile(
                            delete=False, 
                            suffix=".pdf", 
                            dir=self.temp_pdf_dir,
                            prefix=filename.replace(".pdf", "_")
                        )
                        
                        try:
                            # åˆ†å—å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
                            total_size = 0
                            with open(temp_file.name, "wb") as f:
                                for chunk in pdf_response.iter_content(chunk_size=8192):
                                    if chunk:  # è¿‡æ»¤æ‰keep-aliveçš„ç©ºå—
                                        f.write(chunk)
                                        total_size += len(chunk)
                                        
                                        # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
                                        if total_size > self.max_pdf_size:
                                            if self.log_queue:
                                                self.log_queue.put(f"ä¸‹è½½è¿‡ç¨‹ä¸­æ–‡ä»¶è¶…è¿‡å¤§å°é™åˆ¶ï¼Œåœæ­¢ä¸‹è½½")
                                            os.remove(temp_file.name)  # åˆ é™¤ä¸å®Œæ•´çš„æ–‡ä»¶
                                            return result
                            
                            result["pdf_path"] = temp_file.name
                            
                            if self.log_queue:
                                self.log_queue.put(f"PDFä¸‹è½½å®Œæˆ: {temp_file.name} (å¤§å°: {total_size} bytes)")
                                
                        except Exception as e:
                            # ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            if os.path.exists(temp_file.name):
                                os.remove(temp_file.name)
                            if self.log_queue:
                                self.log_queue.put(f"PDFä¸‹è½½å¤±è´¥: {str(e)}")
                            
                    else:
                        if self.log_queue:
                            self.log_queue.put(f"é“¾æ¥ä¸æ˜¯æœ‰æ•ˆçš„PDFæ–‡ä»¶ (Content-Type: {content_type})")
                else:
                    if self.log_queue:
                        self.log_queue.put(f"PDFé“¾æ¥éªŒè¯å¤±è´¥ (çŠ¶æ€ç : {head_response.status_code})")
                        
            except requests.RequestException as e:
                if self.log_queue:
                    self.log_queue.put(f"PDFä¸‹è½½è¯·æ±‚å¤±è´¥: {str(e)}")
            except Exception as e:
                if self.log_queue:
                    self.log_queue.put(f"PDFå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        else:
            if self.log_queue:
                self.log_queue.put("æœªæ‰¾åˆ°æœ‰æ•ˆçš„PDFä¸‹è½½é“¾æ¥")
        
        return result
        
class DailyPapersCollectorTool(BaseTool):
    """
    æ¯æ—¥è®ºæ–‡æ”¶é›†å·¥å…· - æ‰¹é‡æ”¶é›†æ¯æ—¥è®ºæ–‡åˆ—è¡¨
    
    ä½œç”¨ï¼š
    1. ä»HuggingFace Papersç­‰ç½‘ç«™æ‰¹é‡æ”¶é›†æ¯æ—¥è®ºæ–‡
    2. å†…éƒ¨ä½¿ç”¨SinglePaperExtractionToolå¤„ç†å•ç¯‡è®ºæ–‡æå–
    3. æ”¯æŒé™åˆ¶æ”¶é›†æ•°é‡å’Œè‡ªå®šä¹‰æ•°æ®æº
    4. æä¾›æ‰¹é‡å¤„ç†çš„è¿›åº¦åé¦ˆå’Œé”™è¯¯å¤„ç†
    """
    
    def __init__(self, log_queue=None):
        """
        åˆå§‹åŒ–æ¯æ—¥è®ºæ–‡æ”¶é›†å·¥å…·
        
        å‚æ•°:
            log_queue: æ—¥å¿—é˜Ÿåˆ—ï¼Œç”¨äºå‘ä¸»è¿›ç¨‹å‘é€æ—¥å¿—ä¿¡æ¯
        """
        super().__init__(log_queue)
        self.single_extractor = SinglePaperExtractionTool(log_queue)
        self.default_source_url = "https://huggingface.co/papers"
    
    def get_metadata(self) -> ToolMetadata:
        """
        è·å–å·¥å…·å…ƒæ•°æ®
        
        ä½œç”¨ï¼š
        1. å®šä¹‰æ‰¹é‡æ”¶é›†å·¥å…·çš„å‚æ•°å’ŒåŠŸèƒ½
        2. è®©Agentäº†è§£å¦‚ä½•é…ç½®æ‰¹é‡æ”¶é›†ä»»åŠ¡
        3. æ”¯æŒçµæ´»çš„æ•°æ®æºå’Œæ•°é‡æ§åˆ¶
        
        è¿”å›:
            ToolMetadata: åŒ…å«å·¥å…·åç§°ã€æè¿°ã€å‚æ•°å®šä¹‰ç­‰ä¿¡æ¯
        """
        # TODO: å®ç°å…ƒæ•°æ®å®šä¹‰
        # è¿”å› ToolMetadata å¯¹è±¡ï¼ŒåŒ…å«ï¼š
        # - name: "daily_papers_collector"
        # - description: "ä»HuggingFace Papersé¡µé¢æ‰¹é‡æ”¶é›†æ¯æ—¥è®ºæ–‡ä¿¡æ¯"
        # - parameters: {
        #     "source_url": {"type": "str", "required": False, "description": "è®ºæ–‡åˆ—è¡¨é¡µé¢URLï¼Œé»˜è®¤ä¸ºHuggingFace Papers"},
        #     "max_count": {"type": "int", "required": False, "description": "æœ€å¤§æ”¶é›†æ•°é‡ï¼Œä¸è®¾ç½®åˆ™æ”¶é›†æ‰€æœ‰"},
        #     "include_pdf": {"type": "bool", "required": False, "description": "æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue"}
        #   }
        # - return_type: "list"
        # - category: "extraction"
        pass
    
    def _execute_impl(self, **kwargs) -> list[Dict[str, Any]]:
        """
        æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ - æ‰¹é‡æ”¶é›†è®ºæ–‡ä¿¡æ¯
        
        ä½œç”¨ï¼š
        1. å®ç°æ‰¹é‡è®ºæ–‡æ”¶é›†çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
        2. è§£æè®ºæ–‡åˆ—è¡¨é¡µé¢ï¼Œè·å–æ‰€æœ‰è®ºæ–‡é“¾æ¥
        3. ä½¿ç”¨SinglePaperExtractionToolå¤„ç†æ¯ç¯‡è®ºæ–‡
        4. æä¾›è¿›åº¦åé¦ˆå’Œé”™è¯¯å¤„ç†
        
        å‚æ•°:
            source_url (str, optional): è®ºæ–‡åˆ—è¡¨é¡µé¢URL
            max_count (int, optional): æœ€å¤§æ”¶é›†æ•°é‡
            include_pdf (bool, optional): æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶
            
        è¿”å›:
            List[Dict[str, Any]]: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            {
                "title": "è®ºæ–‡æ ‡é¢˜",
                "abstract": "è®ºæ–‡æ‘˜è¦",
                "pdf_path": "PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸‹è½½ï¼‰",
                "url": "è®ºæ–‡é¡µé¢URL",
                "extraction_status": "success/failed",
                "error_message": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"
            }
        
        å®ç°é€»è¾‘ï¼š
        1. è·å–å¹¶éªŒè¯è¾“å…¥å‚æ•°
        2. å‘é€HTTPè¯·æ±‚è·å–è®ºæ–‡åˆ—è¡¨é¡µé¢
        3. è§£æHTMLï¼Œæå–æ‰€æœ‰è®ºæ–‡é“¾æ¥
        4. æ ¹æ®max_counté™åˆ¶è®ºæ–‡æ•°é‡
        5. å¾ªç¯è°ƒç”¨SinglePaperExtractionToolå¤„ç†æ¯ç¯‡è®ºæ–‡
        6. è®°å½•å¤„ç†è¿›åº¦å’Œé”™è¯¯ä¿¡æ¯
        7. è¿”å›å®Œæ•´çš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # TODO: å®ç°æ‰¹é‡æ”¶é›†é€»è¾‘
        # 1. è·å–å‚æ•°ï¼šsource_url, max_count, include_pdf
        # 2. å‘é€requests.get()è¯·æ±‚è·å–åˆ—è¡¨é¡µé¢
        # 3. åˆ›å»ºBeautifulSoupå¯¹è±¡è§£æHTML
        # 4. æå–æ‰€æœ‰è®ºæ–‡é“¾æ¥ï¼ˆæŸ¥æ‰¾articleæ ‡ç­¾æˆ–ç›¸å…³ç»“æ„ï¼‰
        # 5. æ ¹æ®max_counté™åˆ¶å¤„ç†æ•°é‡
        # 6. å¾ªç¯å¤„ç†æ¯ç¯‡è®ºæ–‡ï¼š
        #    - è°ƒç”¨self.single_extractor.execute()
        #    - è®°å½•å¤„ç†è¿›åº¦
        #    - å¤„ç†å¼‚å¸¸æƒ…å†µ
        #    - æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
        # 7. è¿”å›æ‰€æœ‰è®ºæ–‡ä¿¡æ¯çš„åˆ—è¡¨
        pass
    
    def validate_parameters(self, **kwargs) -> bool:
        """
        éªŒè¯è¾“å…¥å‚æ•°
        
        ä½œç”¨ï¼š
        1. éªŒè¯source_urlæ ¼å¼ï¼ˆå¦‚æœæä¾›ï¼‰
        2. æ£€æŸ¥max_countæ˜¯å¦ä¸ºæ­£æ•´æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        3. éªŒè¯include_pdfæ˜¯å¦ä¸ºå¸ƒå°”å€¼ï¼ˆå¦‚æœæä¾›ï¼‰
        4. ç¡®ä¿å‚æ•°ç»„åˆçš„åˆç†æ€§
        
        å®ç°é€»è¾‘ï¼š
        1. æ£€æŸ¥source_urlæ ¼å¼ï¼ˆå¯é€‰å‚æ•°ï¼‰
        2. éªŒè¯max_countèŒƒå›´ï¼ˆå¯é€‰å‚æ•°ï¼Œå¿…é¡»>0ï¼‰
        3. éªŒè¯include_pdfç±»å‹ï¼ˆå¯é€‰å‚æ•°ï¼‰
        4. æ£€æŸ¥å‚æ•°ä¹‹é—´çš„é€»è¾‘å…³ç³»
        
        è¿”å›:
            bool: å‚æ•°éªŒè¯æ˜¯å¦é€šè¿‡
        """
        # TODO: å®ç°å‚æ•°éªŒè¯
        # 1. éªŒè¯source_urlæ ¼å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # 2. æ£€æŸ¥max_countæ˜¯å¦ä¸ºæ­£æ•´æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # 3. éªŒè¯include_pdfæ˜¯å¦ä¸ºå¸ƒå°”å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # 4. æ£€æŸ¥å‚æ•°çš„åˆç†æ€§
        pass
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨
        
        ä½œç”¨ï¼š
        1. æ£€æŸ¥SinglePaperExtractionToolæ˜¯å¦å¯ç”¨
        2. éªŒè¯ç½‘ç»œè¿æ¥å’Œä¾èµ–åŒ…
        3. ç¡®è®¤æ‰¹é‡å¤„ç†çš„ç¯å¢ƒå‡†å¤‡
        
        å®ç°é€»è¾‘ï¼š
        1. è°ƒç”¨self.single_extractor.is_available()
        2. æ£€æŸ¥ç½‘ç»œè¿æ¥
        3. éªŒè¯å¿…è¦çš„ä¾èµ–åŒ…
        4. æ£€æŸ¥ç³»ç»Ÿèµ„æºï¼ˆå†…å­˜ã€ç£ç›˜ç©ºé—´ï¼‰
        
        è¿”å›:
            bool: å·¥å…·æ˜¯å¦å¯ç”¨
        """
        # TODO: å®ç°å¯ç”¨æ€§æ£€æŸ¥
        # 1. æ£€æŸ¥SinglePaperExtractionToolå¯ç”¨æ€§
        # 2. éªŒè¯ç½‘ç»œå’Œä¾èµ–
        # 3. æ£€æŸ¥ç³»ç»Ÿèµ„æº
        pass
    
    def get_usage_example(self) -> Dict[str, Any]:
        """
        è·å–å·¥å…·ä½¿ç”¨ç¤ºä¾‹
        
        ä½œç”¨ï¼š
        1. ä¸ºAgentæä¾›æ‰¹é‡æ”¶é›†çš„ä½¿ç”¨ç¤ºä¾‹
        2. å±•ç¤ºä¸åŒå‚æ•°ç»„åˆçš„æ•ˆæœ
        3. è¯´æ˜æ‰¹é‡å¤„ç†çš„é¢„æœŸè¾“å‡ºæ ¼å¼
        
        è¿”å›:
            Dict[str, Any]: åŒ…å«ä½¿ç”¨ç¤ºä¾‹çš„å­—å…¸
        """
        return {
            "input_examples": [
                {
                    "description": "æ”¶é›†å‰10ç¯‡è®ºæ–‡ï¼ˆåŒ…å«PDFï¼‰",
                    "params": {
                        "max_count": 10,
                        "include_pdf": True
                    }
                },
                {
                    "description": "æ”¶é›†æ‰€æœ‰è®ºæ–‡ï¼ˆä»…æ‘˜è¦ï¼Œä¸ä¸‹è½½PDFï¼‰",
                    "params": {
                        "include_pdf": False
                    }
                },
                {
                    "description": "ä»è‡ªå®šä¹‰URLæ”¶é›†è®ºæ–‡",
                    "params": {
                        "source_url": "https://custom-papers-site.com",
                        "max_count": 5
                    }
                }
            ],
            "expected_output": {
                "description": "è¿”å›è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å®Œæ•´çš„è®ºæ–‡æ•°æ®",
                "example": [
                    {
                        "title": "è®ºæ–‡æ ‡é¢˜1",
                        "abstract": "è®ºæ–‡æ‘˜è¦1...",
                        "pdf_path": "/path/to/paper1.pdf",
                        "url": "https://huggingface.co/papers/1",
                        "extraction_status": "success"
                    },
                    {
                        "title": "è®ºæ–‡æ ‡é¢˜2",
                        "abstract": "æå–å¤±è´¥",
                        "pdf_path": None,
                        "url": "https://huggingface.co/papers/2",
                        "extraction_status": "failed",
                        "error_message": "ç½‘ç»œè¿æ¥è¶…æ—¶"
                    }
                ]
            },
            "use_cases": [
                "æ¯æ—¥è‡ªåŠ¨æ”¶é›†æœ€æ–°è®ºæ–‡",
                "æ‰¹é‡æ„å»ºè®ºæ–‡æ•°æ®åº“",
                "ä¸ºç ”ç©¶å›¢é˜Ÿæä¾›è®ºæ–‡æ‘˜è¦æ±‡æ€»",
                "å®šæœŸç›‘æ§ç‰¹å®šé¢†åŸŸçš„æ–°è®ºæ–‡"
            ]
        }
    
    def cleanup(self):
        """
        æ¸…ç†å·¥å…·èµ„æº
        
        ä½œç”¨ï¼š
        1. æ¸…ç†SinglePaperExtractionToolçš„èµ„æº
        2. æ¸…ç†æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ•°æ®
        3. é‡Šæ”¾ç½‘ç»œè¿æ¥å’Œå†…å­˜èµ„æº
        
        å®ç°é€»è¾‘ï¼š
        1. è°ƒç”¨self.single_extractor.cleanup()
        2. æ¸…ç†æ‰¹é‡å¤„ç†çš„ä¸´æ—¶æ•°æ®
        3. é‡Šæ”¾å…¶ä»–èµ„æº
        """
        # TODO: å®ç°èµ„æºæ¸…ç†
        # 1. æ¸…ç†SinglePaperExtractionToolèµ„æº
        # 2. æ¸…ç†æ‰¹é‡å¤„ç†ä¸´æ—¶æ•°æ®
        # 3. é‡Šæ”¾å…¶ä»–èµ„æº
        pass
    
    def _parse_paper_links_from_soup(self, soup):
        """
        ä»BeautifulSoupå¯¹è±¡ä¸­è§£æè®ºæ–‡é“¾æ¥åˆ—è¡¨
        
        ä½œç”¨ï¼š
        1. æä¾›è®ºæ–‡é“¾æ¥æå–çš„ä¸“é—¨æ–¹æ³•
        2. å¤„ç†ä¸åŒç½‘ç«™çš„é“¾æ¥æ ¼å¼
        3. è¿‡æ»¤å’ŒéªŒè¯é“¾æ¥çš„æœ‰æ•ˆæ€§
        
        å®ç°é€»è¾‘ï¼š
        1. æŸ¥æ‰¾åŒ…å«è®ºæ–‡çš„HTMLå…ƒç´ ï¼ˆå¦‚articleæ ‡ç­¾ï¼‰
        2. æå–æ¯ä¸ªè®ºæ–‡çš„é“¾æ¥å’ŒåŸºæœ¬ä¿¡æ¯
        3. æ„å»ºå®Œæ•´çš„URL
        4. è¿”å›è®ºæ–‡é“¾æ¥åˆ—è¡¨
        """
        # TODO: å®ç°è®ºæ–‡é“¾æ¥è§£æé€»è¾‘
        pass
    
    def _process_single_paper_with_retry(self, paper_url, max_retries=3):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„å•ç¯‡è®ºæ–‡å¤„ç†
        
        ä½œç”¨ï¼š
        1. æä¾›æ›´å¯é çš„å•ç¯‡è®ºæ–‡å¤„ç†
        2. å¤„ç†ç½‘ç»œä¸ç¨³å®šç­‰ä¸´æ—¶é—®é¢˜
        3. è®°å½•é‡è¯•è¿‡ç¨‹å’Œå¤±è´¥åŸå› 
        
        å®ç°é€»è¾‘ï¼š
        1. å°è¯•è°ƒç”¨SinglePaperExtractionTool
        2. å¦‚æœå¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
        3. è®°å½•é‡è¯•æ¬¡æ•°å’Œé”™è¯¯ä¿¡æ¯
        4. è¿”å›å¤„ç†ç»“æœæˆ–é”™è¯¯ä¿¡æ¯
        """
        # TODO: å®ç°å¸¦é‡è¯•çš„è®ºæ–‡å¤„ç†é€»è¾‘
        pass
    
    def get_progress_callback(self):
        """
        è·å–è¿›åº¦å›è°ƒå‡½æ•°
        
        ä½œç”¨ï¼š
        1. ä¸ºAgentæä¾›æ‰¹é‡å¤„ç†çš„è¿›åº¦ä¿¡æ¯
        2. æ”¯æŒå®æ—¶ç›‘æ§å’Œç”¨æˆ·åé¦ˆ
        3. ä¾¿äºè°ƒè¯•å’Œæ€§èƒ½ä¼˜åŒ–
        
        è¿”å›:
            callable: è¿›åº¦å›è°ƒå‡½æ•°
        """
        # TODO: å®ç°è¿›åº¦å›è°ƒé€»è¾‘
        pass


class PaperDataManagerTool(BaseTool):
    """
    è®ºæ–‡æ•°æ®ç®¡ç†å·¥å…· - ç®¡ç†è®ºæ–‡æ•°æ®çš„å­˜å‚¨ã€æ£€ç´¢å’Œç»„ç»‡
    
    ä½œç”¨ï¼š
    1. æä¾›è®ºæ–‡æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨ï¼ˆJSONã€æ•°æ®åº“ç­‰ï¼‰
    2. æ”¯æŒè®ºæ–‡æ•°æ®çš„æŸ¥è¯¢ã€è¿‡æ»¤å’Œæ’åº
    3. ç®¡ç†è®ºæ–‡æ–‡ä»¶çš„ç»„ç»‡ç»“æ„å’Œå…ƒæ•°æ®
    4. æä¾›æ•°æ®å¯¼å…¥å¯¼å‡ºå’Œå¤‡ä»½åŠŸèƒ½
    """
    
    def __init__(self, log_queue=None, storage_path="./data/papers"):
        """
        åˆå§‹åŒ–è®ºæ–‡æ•°æ®ç®¡ç†å·¥å…·
        
        å‚æ•°:
            log_queue: æ—¥å¿—é˜Ÿåˆ—ï¼Œç”¨äºå‘ä¸»è¿›ç¨‹å‘é€æ—¥å¿—ä¿¡æ¯
            storage_path: æ•°æ®å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ä¸º./data/papers
        """
        super().__init__(log_queue)
        self.storage_path = storage_path
        self.metadata_file = f"{storage_path}/metadata.json"
        self.papers_index = {}
        self.supported_formats = ["json", "csv", "sqlite"]
    
    def get_metadata(self) -> ToolMetadata:
        """
        è·å–å·¥å…·å…ƒæ•°æ®
        
        ä½œç”¨ï¼š
        1. å®šä¹‰æ•°æ®ç®¡ç†å·¥å…·çš„å‚æ•°å’ŒåŠŸèƒ½
        2. è®©Agentäº†è§£å¦‚ä½•è¿›è¡Œæ•°æ®æ“ä½œ
        3. æ”¯æŒå¤šç§æ•°æ®æ“ä½œç±»å‹ï¼ˆå­˜å‚¨ã€æŸ¥è¯¢ã€å¯¼å‡ºç­‰ï¼‰
        
        è¿”å›:
            ToolMetadata: åŒ…å«å·¥å…·åç§°ã€æè¿°ã€å‚æ•°å®šä¹‰ç­‰ä¿¡æ¯
        """
        # TODO: å®ç°å…ƒæ•°æ®å®šä¹‰
        # è¿”å› ToolMetadata å¯¹è±¡ï¼ŒåŒ…å«ï¼š
        # - name: "paper_data_manager"
        # - description: "ç®¡ç†è®ºæ–‡æ•°æ®çš„å­˜å‚¨ã€æ£€ç´¢å’Œç»„ç»‡"
        # - parameters: {
        #     "action": {"type": "str", "required": True, "description": "æ“ä½œç±»å‹ï¼šsave/load/query/export/import/delete"},
        #     "data": {"type": "dict", "required": False, "description": "è¦ä¿å­˜çš„è®ºæ–‡æ•°æ®ï¼ˆaction=saveæ—¶å¿…éœ€ï¼‰"},
        #     "query_params": {"type": "dict", "required": False, "description": "æŸ¥è¯¢å‚æ•°ï¼ˆaction=queryæ—¶ä½¿ç”¨ï¼‰"},
        #     "export_format": {"type": "str", "required": False, "description": "å¯¼å‡ºæ ¼å¼ï¼šjson/csv/sqlite"},
        #     "file_path": {"type": "str", "required": False, "description": "æ–‡ä»¶è·¯å¾„ï¼ˆå¯¼å…¥å¯¼å‡ºæ—¶ä½¿ç”¨ï¼‰"}
        #   }
        # - return_type: "dict"
        # - category: "data_management"
        pass
    
    def _execute_impl(self, **kwargs) -> Dict[str, Any]:
        """
        æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ - æ ¹æ®actionæ‰§è¡Œä¸åŒçš„æ•°æ®ç®¡ç†æ“ä½œ
        
        ä½œç”¨ï¼š
        1. å®ç°æ•°æ®ç®¡ç†çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
        2. æ ¹æ®actionå‚æ•°åˆ†å‘åˆ°å…·ä½“çš„æ“ä½œæ–¹æ³•
        3. æä¾›ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œç»“æœæ ¼å¼
        4. æ”¯æŒæ‰¹é‡æ“ä½œå’Œäº‹åŠ¡å¤„ç†
        
        å‚æ•°:
            action (str): æ“ä½œç±»å‹ - save/load/query/export/import/delete
            data (dict, optional): è®ºæ–‡æ•°æ®ï¼ˆä¿å­˜æ—¶ä½¿ç”¨ï¼‰
            query_params (dict, optional): æŸ¥è¯¢å‚æ•°
            export_format (str, optional): å¯¼å‡ºæ ¼å¼
            file_path (str, optional): æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            Dict[str, Any]: æ“ä½œç»“æœï¼Œæ ¼å¼ï¼š
            {
                "success": True/False,
                "action": "æ‰§è¡Œçš„æ“ä½œç±»å‹",
                "result": "å…·ä½“ç»“æœæ•°æ®",
                "message": "æ“ä½œè¯´æ˜",
                "count": "å½±å“çš„è®°å½•æ•°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰"
            }
        
        å®ç°é€»è¾‘ï¼š
        1. è·å–å¹¶éªŒè¯actionå‚æ•°
        2. æ ¹æ®actionåˆ†å‘åˆ°å¯¹åº”çš„ç§æœ‰æ–¹æ³•ï¼š
           - save -> _save_paper_data()
           - load -> _load_paper_data()
           - query -> _query_papers()
           - export -> _export_data()
           - import -> _import_data()
           - delete -> _delete_paper_data()
        3. ç»Ÿä¸€å¤„ç†å¼‚å¸¸å’Œè¿”å›ç»“æœ
        4. æ›´æ–°ç´¢å¼•å’Œå…ƒæ•°æ®
        """
        # TODO: å®ç°æ•°æ®ç®¡ç†é€»è¾‘
        # 1. è·å–actionå‚æ•°
        # 2. æ ¹æ®actionåˆ†å‘åˆ°å…·ä½“æ–¹æ³•
        # 3. å¤„ç†å¼‚å¸¸å’Œè¿”å›ç»Ÿä¸€æ ¼å¼ç»“æœ
        pass
    
    def validate_parameters(self, **kwargs) -> bool:
        """
        éªŒè¯è¾“å…¥å‚æ•°
        
        ä½œç”¨ï¼š
        1. éªŒè¯actionå‚æ•°çš„æœ‰æ•ˆæ€§
        2. æ£€æŸ¥å¿…éœ€å‚æ•°æ˜¯å¦æä¾›
        3. éªŒè¯æ•°æ®æ ¼å¼å’Œæ–‡ä»¶è·¯å¾„
        4. ç¡®ä¿å‚æ•°ç»„åˆçš„åˆç†æ€§
        
        å®ç°é€»è¾‘ï¼š
        1. æ£€æŸ¥actionæ˜¯å¦åœ¨æ”¯æŒçš„æ“ä½œåˆ—è¡¨ä¸­
        2. æ ¹æ®actionéªŒè¯å¯¹åº”çš„å¿…éœ€å‚æ•°
        3. éªŒè¯æ•°æ®æ ¼å¼å’Œæ–‡ä»¶è·¯å¾„çš„æœ‰æ•ˆæ€§
        4. æ£€æŸ¥å‚æ•°ä¹‹é—´çš„é€»è¾‘å…³ç³»
        
        è¿”å›:
            bool: å‚æ•°éªŒè¯æ˜¯å¦é€šè¿‡
        """
        # TODO: å®ç°å‚æ•°éªŒè¯
        # 1. éªŒè¯actionå‚æ•°
        # 2. æ£€æŸ¥å¿…éœ€å‚æ•°
        # 3. éªŒè¯æ•°æ®æ ¼å¼
        # 4. æ£€æŸ¥å‚æ•°é€»è¾‘å…³ç³»
        pass
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨
        
        ä½œç”¨ï¼š
        1. æ£€æŸ¥å­˜å‚¨è·¯å¾„æ˜¯å¦å¯è®¿é—®
        2. éªŒè¯å¿…è¦çš„ä¾èµ–åŒ…
        3. ç¡®è®¤æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        4. æ£€æŸ¥ç£ç›˜ç©ºé—´å’Œæƒé™
        
        å®ç°é€»è¾‘ï¼š
        1. æ£€æŸ¥å­˜å‚¨è·¯å¾„çš„è¯»å†™æƒé™
        2. éªŒè¯JSONã€CSVç­‰å¤„ç†åº“
        3. æµ‹è¯•æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœé…ç½®ï¼‰
        4. æ£€æŸ¥ç£ç›˜ç©ºé—´
        
        è¿”å›:
            bool: å·¥å…·æ˜¯å¦å¯ç”¨
        """
        # TODO: å®ç°å¯ç”¨æ€§æ£€æŸ¥
        # 1. æ£€æŸ¥å­˜å‚¨è·¯å¾„æƒé™
        # 2. éªŒè¯ä¾èµ–åŒ…
        # 3. æµ‹è¯•æ•°æ®åº“è¿æ¥
        # 4. æ£€æŸ¥ç³»ç»Ÿèµ„æº
        pass
    
    def get_usage_example(self) -> Dict[str, Any]:
        """
        è·å–å·¥å…·ä½¿ç”¨ç¤ºä¾‹
        
        ä½œç”¨ï¼š
        1. ä¸ºAgentæä¾›æ•°æ®ç®¡ç†çš„ä½¿ç”¨ç¤ºä¾‹
        2. å±•ç¤ºä¸åŒæ“ä½œç±»å‹çš„å‚æ•°æ ¼å¼
        3. è¯´æ˜é¢„æœŸçš„è¾“å…¥è¾“å‡ºæ ¼å¼
        
        è¿”å›:
            Dict[str, Any]: åŒ…å«ä½¿ç”¨ç¤ºä¾‹çš„å­—å…¸
        """
        return {
            "input_examples": [
                {
                    "description": "ä¿å­˜å•ç¯‡è®ºæ–‡æ•°æ®",
                    "params": {
                        "action": "save",
                        "data": {
                            "title": "è®ºæ–‡æ ‡é¢˜",
                            "abstract": "è®ºæ–‡æ‘˜è¦",
                            "pdf_path": "/path/to/paper.pdf",
                            "url": "https://example.com/paper",
                            "timestamp": "2024-01-01T00:00:00Z"
                        }
                    }
                },
                {
                    "description": "æŸ¥è¯¢åŒ…å«ç‰¹å®šå…³é”®è¯çš„è®ºæ–‡",
                    "params": {
                        "action": "query",
                        "query_params": {
                            "title_contains": "machine learning",
                            "limit": 10,
                            "sort_by": "timestamp",
                            "order": "desc"
                        }
                    }
                },
                {
                    "description": "å¯¼å‡ºæ‰€æœ‰è®ºæ–‡æ•°æ®ä¸ºCSVæ ¼å¼",
                    "params": {
                        "action": "export",
                        "export_format": "csv",
                        "file_path": "./exports/papers.csv"
                    }
                },
                {
                    "description": "ä»JSONæ–‡ä»¶å¯¼å…¥è®ºæ–‡æ•°æ®",
                    "params": {
                        "action": "import",
                        "file_path": "./imports/papers.json"
                    }
                }
            ],
            "expected_output": {
                "description": "è¿”å›æ“ä½œç»“æœï¼ŒåŒ…å«æˆåŠŸçŠ¶æ€å’Œå…·ä½“æ•°æ®",
                "example": {
                    "success": True,
                    "action": "query",
                    "result": [
                        {
                            "id": "paper_001",
                            "title": "æœºå™¨å­¦ä¹ è®ºæ–‡æ ‡é¢˜",
                            "abstract": "è®ºæ–‡æ‘˜è¦...",
                            "timestamp": "2024-01-01T00:00:00Z"
                        }
                    ],
                    "message": "æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ°1ç¯‡è®ºæ–‡",
                    "count": 1
                }
            },
            "use_cases": [
                "æ„å»ºè®ºæ–‡æ•°æ®åº“",
                "è®ºæ–‡æ•°æ®å¤‡ä»½å’Œæ¢å¤",
                "è®ºæ–‡ä¿¡æ¯æ£€ç´¢å’Œè¿‡æ»¤",
                "æ•°æ®åˆ†æå’Œç»Ÿè®¡",
                "ä¸å…¶ä»–ç³»ç»Ÿçš„æ•°æ®äº¤æ¢"
            ]
        }
    
    def cleanup(self):
        """
        æ¸…ç†å·¥å…·èµ„æº
        
        ä½œç”¨ï¼š
        1. ä¿å­˜æœªæäº¤çš„æ•°æ®å˜æ›´
        2. å…³é—­æ•°æ®åº“è¿æ¥
        3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜
        4. é‡Šæ”¾å†…å­˜èµ„æº
        
        å®ç°é€»è¾‘ï¼š
        1. ä¿å­˜papers_indexåˆ°ç£ç›˜
        2. å…³é—­æ•°æ®åº“è¿æ¥
        3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        4. é‡Šæ”¾å†…å­˜
        """
        # TODO: å®ç°èµ„æºæ¸…ç†
        # 1. ä¿å­˜ç´¢å¼•æ•°æ®
        # 2. å…³é—­æ•°æ®åº“è¿æ¥
        # 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        # 4. é‡Šæ”¾å†…å­˜èµ„æº
        pass
    
    def _save_paper_data(self, data):
        """
        ä¿å­˜è®ºæ–‡æ•°æ®åˆ°å­˜å‚¨ç³»ç»Ÿ
        
        ä½œç”¨ï¼š
        1. å°†è®ºæ–‡æ•°æ®æŒä¹…åŒ–å­˜å‚¨
        2. ç”Ÿæˆå”¯ä¸€IDå’Œæ—¶é—´æˆ³
        3. æ›´æ–°ç´¢å¼•å’Œå…ƒæ•°æ®
        4. å¤„ç†é‡å¤æ•°æ®æ£€æŸ¥
        
        å®ç°é€»è¾‘ï¼š
        1. ç”Ÿæˆè®ºæ–‡å”¯ä¸€ID
        2. æ·»åŠ æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
        3. ä¿å­˜åˆ°JSONæ–‡ä»¶æˆ–æ•°æ®åº“
        4. æ›´æ–°ç´¢å¼•
        """
        # TODO: å®ç°æ•°æ®ä¿å­˜é€»è¾‘
        pass
    
    def _load_paper_data(self, paper_id=None):
        """
        åŠ è½½è®ºæ–‡æ•°æ®
        
        ä½œç”¨ï¼š
        1. ä»å­˜å‚¨ç³»ç»ŸåŠ è½½è®ºæ–‡æ•°æ®
        2. æ”¯æŒåŠ è½½å•ç¯‡æˆ–æ‰€æœ‰è®ºæ–‡
        3. å¤„ç†æ•°æ®æ ¼å¼è½¬æ¢
        4. æä¾›ç¼“å­˜æœºåˆ¶
        
        å®ç°é€»è¾‘ï¼š
        1. æ ¹æ®paper_idåŠ è½½ç‰¹å®šè®ºæ–‡æˆ–æ‰€æœ‰è®ºæ–‡
        2. ä»JSONæ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–
        3. æ ¼å¼åŒ–è¿”å›æ•°æ®
        """
        # TODO: å®ç°æ•°æ®åŠ è½½é€»è¾‘
        pass
    
    def _query_papers(self, query_params):
        """
        æŸ¥è¯¢è®ºæ–‡æ•°æ®
        
        ä½œç”¨ï¼š
        1. æ ¹æ®æŸ¥è¯¢æ¡ä»¶è¿‡æ»¤è®ºæ–‡
        2. æ”¯æŒå¤šç§æŸ¥è¯¢æ¡ä»¶ç»„åˆ
        3. æä¾›æ’åºå’Œåˆ†é¡µåŠŸèƒ½
        4. ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
        
        å®ç°é€»è¾‘ï¼š
        1. è§£ææŸ¥è¯¢å‚æ•°
        2. åº”ç”¨è¿‡æ»¤æ¡ä»¶
        3. æ’åºå’Œåˆ†é¡µ
        4. è¿”å›æŸ¥è¯¢ç»“æœ
        """
        # TODO: å®ç°æŸ¥è¯¢é€»è¾‘
        pass
    
    def _export_data(self, export_format, file_path=None):
        """
        å¯¼å‡ºè®ºæ–‡æ•°æ®
        
        ä½œç”¨ï¼š
        1. å°†è®ºæ–‡æ•°æ®å¯¼å‡ºä¸ºæŒ‡å®šæ ¼å¼
        2. æ”¯æŒJSONã€CSVã€SQLiteç­‰æ ¼å¼
        3. å¤„ç†å¤§æ•°æ®é‡çš„åˆ†æ‰¹å¯¼å‡º
        4. æä¾›å¯¼å‡ºè¿›åº¦åé¦ˆ
        
        å®ç°é€»è¾‘ï¼š
        1. éªŒè¯å¯¼å‡ºæ ¼å¼
        2. å‡†å¤‡å¯¼å‡ºæ•°æ®
        3. æ ¹æ®æ ¼å¼è°ƒç”¨å¯¹åº”çš„å¯¼å‡ºæ–¹æ³•
        4. ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        """
        # TODO: å®ç°æ•°æ®å¯¼å‡ºé€»è¾‘
        pass
    
    def _import_data(self, file_path):
        """
        å¯¼å…¥è®ºæ–‡æ•°æ®
        
        ä½œç”¨ï¼š
        1. ä»å¤–éƒ¨æ–‡ä»¶å¯¼å…¥è®ºæ–‡æ•°æ®
        2. æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        3. å¤„ç†æ•°æ®éªŒè¯å’Œå»é‡
        4. æä¾›å¯¼å…¥è¿›åº¦åé¦ˆ
        
        å®ç°é€»è¾‘ï¼š
        1. æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå­˜åœ¨æ€§
        2. è¯»å–å’Œè§£ææ–‡ä»¶æ•°æ®
        3. éªŒè¯æ•°æ®æ ¼å¼
        4. æ‰¹é‡ä¿å­˜åˆ°å­˜å‚¨ç³»ç»Ÿ
        """
        # TODO: å®ç°æ•°æ®å¯¼å…¥é€»è¾‘
        pass
    
    def _delete_paper_data(self, paper_id):
        """
        åˆ é™¤è®ºæ–‡æ•°æ®
        
        ä½œç”¨ï¼š
        1. ä»å­˜å‚¨ç³»ç»Ÿåˆ é™¤æŒ‡å®šè®ºæ–‡
        2. æ¸…ç†ç›¸å…³çš„æ–‡ä»¶å’Œç´¢å¼•
        3. æä¾›è½¯åˆ é™¤å’Œç¡¬åˆ é™¤é€‰é¡¹
        4. è®°å½•åˆ é™¤æ“ä½œæ—¥å¿—
        
        å®ç°é€»è¾‘ï¼š
        1. éªŒè¯è®ºæ–‡IDå­˜åœ¨æ€§
        2. åˆ é™¤è®ºæ–‡æ•°æ®è®°å½•
        3. æ¸…ç†ç›¸å…³æ–‡ä»¶
        4. æ›´æ–°ç´¢å¼•
        """
        # TODO: å®ç°æ•°æ®åˆ é™¤é€»è¾‘
        pass
    
    def get_statistics(self):
        """
        è·å–è®ºæ–‡æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        ä½œç”¨ï¼š
        1. æä¾›è®ºæ–‡æ•°é‡ã€å­˜å‚¨å¤§å°ç­‰ç»Ÿè®¡
        2. åˆ†æè®ºæ–‡æ¥æºå’Œæ—¶é—´åˆ†å¸ƒ
        3. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
        4. æ”¯æŒAgentçš„å†³ç­–åˆ¶å®š
        
        è¿”å›:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # TODO: å®ç°ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆ
        pass
    
    def create_backup(self, backup_path=None):
        """
        åˆ›å»ºæ•°æ®å¤‡ä»½
        
        ä½œç”¨ï¼š
        1. åˆ›å»ºå®Œæ•´çš„æ•°æ®å¤‡ä»½
        2. æ”¯æŒå¢é‡å¤‡ä»½å’Œå…¨é‡å¤‡ä»½
        3. å‹ç¼©å¤‡ä»½æ–‡ä»¶èŠ‚çœç©ºé—´
        4. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
        
        å®ç°é€»è¾‘ï¼š
        1. ç¡®å®šå¤‡ä»½è·¯å¾„å’Œæ–‡ä»¶å
        2. æ”¶é›†æ‰€æœ‰éœ€è¦å¤‡ä»½çš„æ•°æ®
        3. åˆ›å»ºå‹ç¼©å¤‡ä»½æ–‡ä»¶
        4. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
        """
        # TODO: å®ç°å¤‡ä»½åˆ›å»ºé€»è¾‘
        pass
    
    def restore_from_backup(self, backup_path):
        """
        ä»å¤‡ä»½æ¢å¤æ•°æ®
        
        ä½œç”¨ï¼š
        1. ä»å¤‡ä»½æ–‡ä»¶æ¢å¤è®ºæ–‡æ•°æ®
        2. éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§
        3. å¤„ç†æ•°æ®å†²çªå’Œåˆå¹¶
        4. æä¾›æ¢å¤è¿›åº¦åé¦ˆ
        
        å®ç°é€»è¾‘ï¼š
        1. éªŒè¯å¤‡ä»½æ–‡ä»¶
        2. è§£å‹å’Œè¯»å–å¤‡ä»½æ•°æ®
        3. å¤„ç†æ•°æ®å†²çª
        4. æ¢å¤åˆ°å­˜å‚¨ç³»ç»Ÿ
        """
        # TODO: å®ç°å¤‡ä»½æ¢å¤é€»è¾‘
        pass

# æµ‹è¯•ä»£ç  - ç”¨äºéªŒè¯SinglePaperExtractionToolçš„åŠŸèƒ½
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    print("å¼€å§‹æµ‹è¯•SinglePaperExtractionTool...")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ—¥å¿—é˜Ÿåˆ—æ¨¡æ‹Ÿå™¨
    class SimpleLogQueue:
        def put(self, message):
            print(f"[LOG] {message}")
    
    # åˆå§‹åŒ–å·¥å…·
    log_queue = SimpleLogQueue()
    extractor = SinglePaperExtractionTool(log_queue)
    
    # æµ‹è¯•ç”¨çš„è®ºæ–‡URLï¼ˆHuggingFace Papersç¤ºä¾‹ï¼‰
    test_url = "https://huggingface.co/papers/2509.07980"  # å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–æœ‰æ•ˆçš„è®ºæ–‡URL
    
    print(f"\næµ‹è¯•URL: {test_url}")
    print("="*50)
    
    try:
        # è°ƒç”¨_execute_implå‡½æ•°è¿›è¡Œæµ‹è¯•
        result = extractor._execute_impl(
            paper_url=test_url,
            download_pdf=True,  # è®¾ç½®ä¸ºTrueæµ‹è¯•PDFä¸‹è½½
            custom_filename="test_paper"  # è‡ªå®šä¹‰æ–‡ä»¶å
        )
        
        print("\næµ‹è¯•ç»“æœ:")
        print("="*30)
        print(f"æˆåŠŸçŠ¶æ€: {result.get('success')}")
        print(f"è®ºæ–‡æ ‡é¢˜: {result.get('title')}")
        print(f"æ‘˜è¦é•¿åº¦: {len(result.get('abstract', '')) if result.get('abstract') else 0} å­—ç¬¦")
        print(f"PDFè·¯å¾„: {result.get('pdf_path')}")
        print(f"PDF URL: {result.get('pdf_url')}")
        print(f"æå–æ—¶é—´: {result.get('extraction_time')}")
        
        if result.get('error_message'):
            print(f"é”™è¯¯ä¿¡æ¯: {result.get('error_message')}")
        
        # æ˜¾ç¤ºæ‘˜è¦çš„å‰200ä¸ªå­—ç¬¦
        if result.get('abstract'):
            abstract_preview = result.get('abstract')[:200] + "..." if len(result.get('abstract', '')) > 200 else result.get('abstract')
            print(f"\næ‘˜è¦é¢„è§ˆ:\n{abstract_preview}")
        
    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\næµ‹è¯•å®Œæˆ!")